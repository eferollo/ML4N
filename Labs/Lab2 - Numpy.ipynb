{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LAB #2: Numpy\n",
    "\n",
    "## Introduction\n",
    "In this laboratory, you will build your own version of the K-Nearest Neighbors algorithm (a.k.a. KNN) using\n",
    "the NumPy library.\n",
    "## 0 Preliminary steps\n",
    "### 0.1 NumPy\n",
    "Make sure you have the NumPy library installed, its use is strongly recommended for this laboratory.\n",
    "NumPy is the fundamental package for scientific computing with Python. You can read more about it on\n",
    "the official documentation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1ea060a47a6211d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:37:17.216528665Z",
     "start_time": "2023-10-24T10:37:17.171395922Z"
    }
   },
   "id": "9246699975edf562"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0.2 Iris dataset download \n",
    "For this lab, you will need two of the datasets you have already met: Iris and MNIST. Please refer to\n",
    "Laboratory 1 for a complete description of the datasets.\n",
    "Iris. You can download it from:\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad497ed1d0092203"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.676610150Z",
     "start_time": "2023-10-22T17:56:44.512058578Z"
    }
   },
   "id": "a838a5ed77a24051"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Exercises \n",
    "Note that exercises marked with a ($\\star$) are optional, you should focus on completing the other ones first."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef169d9060adb9a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Iris Analysis with Numpy\n",
    "As you might remember from Lab. 1, the Iris dataset collects the measurements of different Iris flowers,\n",
    "and each data point is characterized by 4 **features** (sepal length, sepal width, petal length, petal width) and is associated to 1 **label** (i.e. an Iris species - Setosa, Versicolor, or Virginica)  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a820274dc6b6f678"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Load the Iris dataset. You can use the `csv` library that we saw in the last laboratory or read it with the standard `open(filename, strategy)`. \n",
    "In the second case remember to split correctly the different fields, avoid new line characters. In any case check for empty lines. \n",
    "This time remember to store the 4 features in a numpy array `x` of shape (n_sample, 4) and the labels in a different array `y` of shape (n_sample,) converting the 3 different species to a corresponding numerical value. E.g.,\n",
    "      - Iris-setosa: 0\n",
    "      - Iris-versicolor: 1\n",
    "      - Iris-virginica: 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46864c46cf9f9387"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "x = []\n",
    "y = []\n",
    "with open(\"iris.csv\",\"r\") as iris:\n",
    "    for i,row in enumerate(csv.reader(iris)):\n",
    "        if row == []:\n",
    "            continue\n",
    "        x.append(row[:-1])\n",
    "        y.append(row[-1])\n",
    "    x = np.array(x, dtype='float64')\n",
    "    y = np.array(y)\n",
    "    \n",
    "    y[y == \"Iris-setosa\"] = 0\n",
    "    y[y == \"Iris-versicolor\"] = 1\n",
    "    y[y == \"Iris-virginica\"] = 2\n",
    "    y=y.astype('int64')\n",
    "    print(x)\n",
    "    print(y)\n",
    "          "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:37:23.374651006Z",
     "start_time": "2023-10-24T10:37:23.346648310Z"
    }
   },
   "id": "a977ccc88ef2ca39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Compute again the mean and standard deviation for each class by means of the numpy functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5050d162966956ce"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m4\u001B[39m):\n\u001B[0;32m----> 6\u001B[0m         means[j]\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mmean(\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43mj\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m))\n\u001B[1;32m      7\u001B[0m         std_devs[j]\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mstd(x[np\u001B[38;5;241m.\u001B[39margwhere(y\u001B[38;5;241m==\u001B[39mj), i]))\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(means)\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "means = [[],[],[]]\n",
    "std_devs = [[],[],[]]\n",
    "for j in range(3):\n",
    "    for i in range(4):\n",
    "        means[j].append(np.mean(x[np.argwhere(y==j), i]))\n",
    "        std_devs[j].append(np.std(x[np.argwhere(y==j), i]))\n",
    "print(means)\n",
    "print(std_devs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:39:41.084525731Z",
     "start_time": "2023-10-24T10:39:41.028677848Z"
    }
   },
   "id": "33bfaed602d4bc3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Compute the distances among two samples (e.g., the $35^{th}$ and the $80^{th}$ and the $12^{th}$ and the $14^{th}$) \n",
    "by means of the 'np.linalg.norm' function which computes the euclidean distance. \n",
    "  - Can you guess if the two couples of samples belong to the same species?\n",
    "  - From the mean and standard deviations computed before can you guess which species? "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f84beb708797ba9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "2.908607914449797"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm(s1,s2):\n",
    "    return np.linalg.norm(x[s1]-x[s2])\n",
    "\n",
    "norm(35,80)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.694239588Z",
     "start_time": "2023-10-22T17:56:44.565224299Z"
    }
   },
   "id": "4a47fb722be07fb4"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4de2b1c8798fc98e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 KNN design and implementation\n",
    "In this exercise, you will implement your own version of the K-Nearest Neighbors (KNN) algorithm, and you will use it to assign an\n",
    "Iris species (i.e. a label) to flowers whose species is unknown.\n",
    "\n",
    "The KNN algorithm is straightforward. Suppose that some measurements (e.g., the iris features) and their\n",
    "relative label (e.g., the iris species) of a set of samples are known in advance. \n",
    "\n",
    "<img src=\"https://mlarchive.com/wp-content/uploads/2022/09/img2.png\" width=\"800\">\n",
    "\n",
    "Then, whenever we want to label a new sample, we look at the K most similar points (a.k.a. neighbors) and assign a label accordingly. \n",
    "\n",
    "<img src=\"https://mlarchive.com/wp-content/uploads/2022/09/img1-1.png\" width=\"800\">\n",
    "\n",
    "\n",
    "The simplest solution is using a majority voting scheme: if the majority of the neighbors votes for a label, we will go for it. \n",
    "This approach is naive only at first sight: the local similarity assumed by KNN happens to be roughly true. \n",
    "Even though this reasoning does not generalize well, the KNN provides a valid baseline for your tasks.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dd1f94b256663e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Let’s identify a portion of our data for which we will try to guess the species. Randomly select 20%\n",
    "of the records and store the first four columns (i.e. the features representing each flower) into a\n",
    "two-dimensional numpy array of shape ($N_{test} \\times 4$), you can call it `X_test` and $N_{test}$ is the 20% of the total number of samples.\n",
    "For the same records, store the test label column (i.e. the one with the species values) into another array, namely `y_test`. \n",
    "This is the data that will be used to test the accuracy of your KNN implementation and its correct functioning (i.e. the testing data)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d185976071690ce"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.6 3.  4.1 1.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.7 2.6 3.5 1. ]]\n",
      "[1 0 2 0 2 2 2 1 1 2 0 1 2 0 2 1 2 2 2 2 2 2 0 0 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(150)\n",
    "np.random.shuffle(indices)\n",
    "train_len = 80*len(indices) // 100\n",
    "test_idx = indices[train_len:]\n",
    "\n",
    "X_test = x[test_idx]\n",
    "y_test = y[test_idx]\n",
    "print(X_test)\n",
    "print(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.697075749Z",
     "start_time": "2023-10-22T17:56:44.569968093Z"
    }
   },
   "id": "a642f03b563650e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Store the remaining 80% of the records in the same way. In this case, use the names X_train andy_train for the arrays.\n",
    "This is the data that your model will use as ground-truth knowledge (i.e. the training data, from which we extract the knowledge and that we will use for comparison).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192e5663358e8e82"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.2 2.2 4.5 1.5]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.  2.2 4.  1. ]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.1 3.5 1.4 0.3]]\n",
      "[1 2 1 2 0 0 1 2 1 2 1 1 1 0 1 0 2 1 0 2 2 2 0 2 1 1 1 0 2 1 0 1 2 0 1 0 2\n",
      " 1 2 2 0 0 0 1 2 0 1 2 1 0 2 0 1 0 2 0 0 1 1 0 0 2 0 2 1 0 1 1 0 2 2 1 1 0\n",
      " 2 0 2 2 1 0 2 1 2 1 0 1 2 2 2 0 1 2 2 0 0 1 0 1 1 1 2 0 1 2 1 0 0 0 1 2 1\n",
      " 2 0 0 1 0 0 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "train_idx = indices[:train_len]\n",
    "X_train = x[train_idx]\n",
    "y_train = y[train_idx]\n",
    "print(X_train)\n",
    "print(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.699895130Z",
     "start_time": "2023-10-22T17:56:44.580994831Z"
    }
   },
   "id": "b9f1639cc7fe3b53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Focus now on the KNN technique. \n",
    "From the next month, you will use the `scikit-learn` package. Many of its functionalities\n",
    "are exposed via an object-oriented interface. With this paradigm in mind, implement now the KNN\n",
    "algorithm and expose it as a Python class. The bare skeleton of your class should look like this (you\n",
    "are free to add other methods if you want to).\n",
    "\n",
    "```\n",
    "class KNearestNeighbors:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Store the 'prior knowledge' of you model that will be used\n",
    "        to predict new labels.\n",
    "        :param X : input data points, ndarray, shape = (R,C).\n",
    "        :param y : input labels, ndarray, shape = (R,).\n",
    "        \"\"\"\n",
    "        pass # TODO: implement it!\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Run the KNN classification on X.\n",
    "        :param X: input data points, ndarray, shape = (N,C).\n",
    "        :return: labels : ndarray, shape = (N,).\n",
    "        \"\"\"\n",
    "        pass # TODO: implement it!\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Implement the fit method first. Here, you should only keep track of the main attributes that will be used by the algorithm.\n",
    "In this version of the algorithm, does the KNN need to store all the samples of `X_train` and `y_train`?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbbc62af2fef1d5c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#yes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.701269558Z",
     "start_time": "2023-10-22T17:56:44.585865959Z"
    }
   },
   "id": "b5de6a78df7f8585"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Implement the `predict` method. The function receives as input a numpy array with N rows and C\n",
    "columns, corresponding to N flowers. The method assigns to each row one of the three Iris species \n",
    "using the KNN algorithm, and returns the predicted species as a numpy array. For the actual implementation, apply\n",
    "the identify K neighbors using the euclidean distance specified by the parameters k.\n",
    "Then, assign the label using a majority voting scheme\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad6f4fc7071bff0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n",
      "[2]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[1]\n",
      "[1]\n",
      "[2]\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[0]\n",
      "[2]\n",
      "[1]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[0]\n",
      "[1]\n",
      "[0]\n",
      "[1]\n",
      "[1 0 2 0 2 2 2 1 1 2 0 1 2 0 2 1 2 2 2 2 2 1 0 0 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "class KNearestNeighbors:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    def fit(self, X, y):\n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        out = []\n",
    "        for i in range(len(X)):\n",
    "            fdist = []\n",
    "            votes = []\n",
    "            for j in range(len(X_train)):\n",
    "                dist = np.linalg.norm(self.X_train[j]-X[i])\n",
    "                fdist.append([dist, j])\n",
    "            fdist.sort()\n",
    "            fdist = fdist[0:self.k]\n",
    "            for d,j in fdist:\n",
    "                votes.append(y_train[j])\n",
    "            print(votes)\n",
    "            out.append(Counter(votes).most_common(1)[0][0])\n",
    "        return np.array(out)\n",
    "    \n",
    "    def accuracy(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        print(f\"Accuracy score: {(y_pred==y_test).sum()/len(y_test)}\")\n",
    "        \n",
    "KNN = KNearestNeighbors(1)\n",
    "KNN.fit(X_train,y_train)\n",
    "\n",
    "print(KNN.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.739769966Z",
     "start_time": "2023-10-22T17:56:44.599612864Z"
    }
   },
   "id": "c227627e47cc7253"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "87643f049175398"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Now let’s fit the KNN model with the X_train and y_train data. Then, try to use your KNN model\n",
    "to predict the species for each record in X_test and store them in a nupy array called y_pred.\n",
    "As we did in the previous lab, check how many Iris species in the array y_pred have been guessed correctly computing with respect to the ones in y_test computing the accuracy. \n",
    "A prediction is correct if `y_pred[i] == y_test[i]`. To get the accuracy then compute the ratio between the number of correct guesses and the total number of guesses is known. \n",
    "If all labels are assigned correctly ((y_pred == y_test).all() == True), the accuracy of the model is 100%. \n",
    "Instead, if none of the guessed species corresponds to the real one ((y_pred == y_test).any() == False), the accuracy is 0%\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cbd1131d3ba785d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n",
      "[2]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[1]\n",
      "[1]\n",
      "[2]\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[0]\n",
      "[2]\n",
      "[1]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[0]\n",
      "[1]\n",
      "[0]\n",
      "[1]\n",
      "Accuracy score: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "KNN.accuracy(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.740033814Z",
     "start_time": "2023-10-22T17:56:44.642421615Z"
    }
   },
   "id": "ca4f0b4bbe44c9fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. ($\\star$) As a software developer, you might want to increase the functionalities of your product and\n",
    "publish newer versions over time. The better your code is structured and organized, the lower is the\n",
    "effort to release updates.\n",
    "As such,  extend your KNN implementation adding the parameter `distance`. This has to be one among:\n",
    "    - Euclidean distance: $ euclidean(p,q) = \\sqrt{\\sum_{i=1}^{n} (p_i _- q_i)^2} $\n",
    "    - Manhattan distance: $ manhattan(p,q) = \\sum_{i=1}^n |p_i - q_i|$\n",
    "    - Cosine distance: $ cosine(p, q) = \\frac{\\sum_{i=1}^n p_i q_i}{ \\sqrt{\\sum^n_{i=1}} p^2_i \\cdot \\sqrt{\\sum^n_{i=1} q_i^2}}$\n",
    "\n",
    "If any of this distance is not already implemented in `numpy` implement it yourself"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7514fc82de74b729"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1732050807568884, 6], [0.22360679774997935, 57], [0.31622776601683794, 104]]\n",
      "[[0.09999999999999964, 40], [0.14142135623730964, 51], [0.17320508075688762, 59]]\n",
      "[[0.17320508075688762, 61], [0.24494897427831777, 87], [0.3605551275463989, 9]]\n",
      "[[0.19999999999999993, 30], [0.19999999999999993, 53], [0.22360679774997896, 101]]\n",
      "[[0.22360679774997935, 103], [0.26457513110645947, 100], [0.2999999999999998, 16]]\n",
      "[[0.3872983346207416, 21], [0.43588989435406783, 23], [0.5000000000000001, 100]]\n",
      "[[0.5567764362830021, 54], [0.6164414002968976, 76], [0.6164414002968976, 111]]\n",
      "[[0.33166247903554, 52], [0.3464101615137753, 10], [0.37416573867739383, 8]]\n",
      "[[0.14142135623730995, 11], [0.24494897427831722, 34], [0.2645751311064587, 46]]\n",
      "[[0.3000000000000001, 21], [0.5567764362830023, 70], [0.6164414002968975, 50]]\n",
      "[[0.2999999999999998, 75], [0.412310562561766, 22], [0.4242640687119283, 55]]\n",
      "[[0.24494897427831766, 8], [0.2828427124746188, 10], [0.3316624790355398, 57]]\n",
      "[[0.5000000000000001, 23], [0.5567764362830022, 103], [0.6082762530298224, 100]]\n",
      "[[0.1414213562373093, 55], [0.17320508075688812, 4], [0.22360679774997858, 75]]\n",
      "[[0.38729833462074187, 88], [0.43588989435406733, 7], [0.46904157598234325, 47]]\n",
      "[[0.14142135623730964, 6], [0.14142135623730995, 57], [0.22360679774997896, 104]]\n",
      "[[0.30000000000000016, 76], [0.3162277660168381, 54], [0.360555127546399, 100]]\n",
      "[[0.26457513110645964, 109], [0.4123105625617659, 20], [0.6082762530298225, 7]]\n",
      "[[0.31622776601683755, 19], [0.31622776601683755, 32], [0.33166247903553986, 63]]\n",
      "[[0.5099019513592784, 47], [0.5196152422706635, 88], [0.5567764362830021, 7]]\n",
      "[[0.24494897427831783, 111], [0.24494897427831802, 69], [0.3316624790355402, 54]]\n",
      "[[0.33166247903553975, 98], [0.3605551275463984, 78], [0.43588989435406716, 61]]\n",
      "[[0.3, 113], [0.31622776601683783, 42], [0.3464101615137756, 84]]\n",
      "[[0.14142135623730917, 119], [0.17320508075688743, 59], [0.17320508075688767, 15]]\n",
      "[[0.0, 30], [0.0, 53], [0.17320508075688784, 101]]\n",
      "[[0.26457513110645914, 104], [0.3162277660168378, 43], [0.42426406871192884, 2]]\n",
      "[[0.14142135623730948, 45], [0.2999999999999997, 4], [0.31622776601683816, 93]]\n",
      "[[0.19999999999999973, 25], [0.24494897427831785, 90], [0.33166247903553975, 83]]\n",
      "[[0.14142135623730964, 49], [0.14142135623730995, 40], [0.14142135623730995, 59]]\n",
      "[[0.3464101615137758, 66], [0.42426406871192857, 72], [0.43588989435406744, 114]]\n",
      "[1 0 2 0 2 2 2 1 1 2 0 1 2 0 2 1 2 2 2 2 2 1 0 0 0 1 0 1 0 1]\n",
      "Accuracy score: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance as ss\n",
    "from collections import Counter\n",
    "class KNearestNeighbors1:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    def fit(self, X, y):\n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "        \n",
    "    def distance(self,opt,x,y):\n",
    "        if opt==0:\n",
    "            return np.linalg.norm(x-y)\n",
    "        if opt == 1:\n",
    "            return abs(x-y).sum()\n",
    "        if opt == 2:\n",
    "            return ss.cosine(x,y)\n",
    "    \n",
    "    def predict(self, X, opt):\n",
    "        out = []\n",
    "        for i in range(len(X)):\n",
    "            fdist = []\n",
    "            votes = []\n",
    "            for j in range(len(X_train)):\n",
    "                dist = self.distance(opt,self.X_train[j],X[i])\n",
    "                fdist.append([dist, j])\n",
    "            fdist.sort()\n",
    "            fdist = fdist[0:self.k]\n",
    "            print(fdist)\n",
    "            for d,j in fdist:\n",
    "                votes.append(y_train[j])\n",
    "            out.append(Counter(votes).most_common(1)[0][0])\n",
    "        return np.array(out)\n",
    "    \n",
    "    def accuracy(self, X_test, y_test, opt):\n",
    "        y_pred = self.predict(X_test, opt)\n",
    "        print(y_pred)\n",
    "        print(f\"Accuracy score: {(y_pred==y_test).sum()/len(y_test)}\")\n",
    "        \n",
    "KNN = KNearestNeighbors1(3)\n",
    "KNN.fit(X_train,y_train)\n",
    "'''\n",
    "0 - euclidean distance\n",
    "1 - manhattan distance \n",
    "2 - cosine distance\n",
    "'''\n",
    "KNN.accuracy(X_test, y_test, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.828976036Z",
     "start_time": "2023-10-22T17:56:44.663725746Z"
    }
   },
   "id": "436c6395a2f3d853"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "7. ($\\star$) Again, extend now your KNN implementation by adding the parameter `weights` to the constructor,\n",
    "as shown below:\n",
    "\n",
    "```\n",
    "class KNearestNeighbors:\n",
    "def __init__(self, k, distance_metric=\"euclidean\", weights=\"uniform\"):\n",
    "self.k = k\n",
    "self.distance_metric = distance_metric\n",
    "self.weights = weights\n",
    "```\n",
    "\n",
    "Change your KNN implementation to accept a new weighting scheme for the labels. If weights=\n",
    "\"distance\", weight neighbor votes by the inverse of their distance (for the distance, again, use\n",
    "distance_metric). The weight for a neighbor of the point p is:\n",
    "\n",
    "$\n",
    "w(p, n) = \\frac{1}{distance\\_metric(p, n)}\n",
    "$\n",
    "\n",
    "Instead, if the default is chosen (weights=\"uniform\"), use the majority voting you already implemented\n",
    "in Exercise 6.\n",
    "\n",
    "<img src=\"https://mlarchive.com/wp-content/uploads/2022/09/img5.png\">\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c76d735fe65dbd"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 0 2 2 2 1 1 2 0 1 2 0 2 1 2 2 2 2 2 1 0 0 0 1 0 1 0 1]\n",
      "Accuracy score: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance as ss\n",
    "from collections import Counter\n",
    "class KNearestNeighbors2:\n",
    "    def __init__(self, k, distance_metric=\"euclidean\", weights=\"uniform\"):\n",
    "        self.k = k\n",
    "        self.distance_metrics=distance_metric.lower()\n",
    "        self.weights=weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "        \n",
    "    def distance(self,x,y):\n",
    "        if self.distance_metrics == 'euclidean':\n",
    "            return np.linalg.norm(x-y)\n",
    "        if self.distance_metrics == 'manhattan':\n",
    "            return abs(x-y).sum()\n",
    "        if self.distance_metrics == 'cosine':\n",
    "            return ss.cosine(x,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        out = []\n",
    "        for i in range(len(X)):\n",
    "            fdist = []\n",
    "            votes = []\n",
    "            flag = False\n",
    "            for j in range(len(X_train)):\n",
    "                \n",
    "                if self.weights == 'uniform': \n",
    "                    flag = True\n",
    "                    weight = self.distance(self.X_train[j],X[i])\n",
    "                else: \n",
    "                    try:\n",
    "                        weight = (1/self.distance(self.X_train[j],X[i]))\n",
    "                    except: \n",
    "                        weight = 1\n",
    "                fdist.append([weight, j])\n",
    "                \n",
    "            if flag == True: fdist.sort()\n",
    "            else: fdist.sort(reverse=True)\n",
    "            \n",
    "            fdist = fdist[0:self.k]\n",
    "            for d,j in fdist:\n",
    "                votes.append(y_train[j])  \n",
    "            out.append(Counter(votes).most_common(1)[0][0])\n",
    "        return np.array(out)\n",
    "    \n",
    "    def accuracy(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        print(y_pred)\n",
    "        print(f\"Accuracy score: {(y_pred==y_test).sum()/len(y_test)}\")\n",
    "        \n",
    "KNN = KNearestNeighbors2(3,\"manhattan\", \"uniform\")\n",
    "KNN.fit(X_train,y_train)\n",
    "'''\n",
    "0 - euclidean distance\n",
    "1 - manhattan distance \n",
    "2 - cosine distance\n",
    "'''\n",
    "KNN.accuracy(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T18:02:57.856202737Z",
     "start_time": "2023-10-22T18:02:57.805964026Z"
    }
   },
   "id": "a84262b9fd13d9f1"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6e344f25a375e4de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. ($\\star$) Test the modularity of the implementation applying it on a different dataset. Ideally, you should\n",
    "not change the code of your KNN python class.\n",
    "- Download the MNIST dataset and sample only 100 points per digit. You will end up with a dataset of 1000 samples.\n",
    "- Define again four numpy arrays as you did in Exercises 2 and 3.\n",
    "- Apply your KNN as you did for the Iris dataset.\n",
    "- Evaluate the accuracy on MNIST’s y_test."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54f1e2a662695741"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T17:56:44.851472143Z",
     "start_time": "2023-10-22T17:56:44.713145117Z"
    }
   },
   "id": "b720ef714195eb68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
