{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d4e032-42ea-4f38-9a6d-f21cd3fe3610",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Lab-5 Dimensionality Reduction<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89666add-9e84-432b-9c03-2e2ad0ae3cb1",
   "metadata": {},
   "source": [
    "### Some notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4bd99-9f53-4bc2-9488-d458d0d1db86",
   "metadata": {},
   "source": [
    "1. You can use <button>Control</button> + <button>Enter</button> to execute the current code block, and you can use <button>Shift</button> + <button>Enter</button> to execute the current block and go to the next one afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e1f307-2e12-4de3-ab8d-39f4d107d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fc376-bc3b-4e5e-a6c3-313bb998fc27",
   "metadata": {},
   "source": [
    "2. Jupyter notebook automatically outputs the variable at the last line of a block, so you can display a pandas dataframe in a clear way instead of printing it, by putting the variable at the bottom of a block or creating a new block and putting the variable in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96159c4-bb99-41d5-aa74-af6e8b51cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('darknet_traces.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec1fa1-61f3-4e5b-8a80-5b2e5b1b45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b426a2-1a96-4ae5-bc0b-2e8346407db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01aa78f-4552-4c8e-ae5d-08921ed9c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35b8cf-05bd-4bf3-9c34-23dbffed4cad",
   "metadata": {},
   "source": [
    "3. You are not restricted to Seaborn to make plot, and you can still use Matplotlib for your work, just choose the one that suits you the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47710e7-f4c0-4445-b760-368170f4fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = [i for i in range(100)]\n",
    "plt.plot(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a7a98-683a-46bd-b957-733269980379",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65120f2c-9c08-4fb7-ab28-8ad154816c74",
   "metadata": {},
   "source": [
    "4. ``pd.to_datetime()`` works with nanosecond, and this is why we multiply the second by $10^9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb632c0b-68f4-42cf-9d23-8f958dfb81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 1623452345.3400002\n",
    "pd.to_datetime(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48894a6-bf15-4f5c-91f7-0e1cc266b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(ts * 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6983a50-b719-4951-a1f3-fa41f2ac0db4",
   "metadata": {},
   "source": [
    "5. All the code blocks that you have executed will affect the variables, so if you run a certain block again, the variables might be further changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38ce24-e946-4a6f-9f0e-e81f206950a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ts'] = df['ts'] * 1e9\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b0831-9cc6-480d-880b-f8f3399d38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy['ts'] = df_copy['ts'] * 1e9\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723705c-d539-4132-9852-0c64472c5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('ts')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96df08c-25ca-4e53-927c-c52c277bf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('ts') \n",
    "# df.set_index('ts', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18dff18-fcc7-498f-9c31-c5fa64d52afa",
   "metadata": {},
   "source": [
    "### Objective: Applying following techniques\n",
    "1. **Scikit-learn (sklearn)** is one of the most famous Python libraries designed for machine learning. You can use it to do literally everything for ML, such as classification and clustering. In this tutorial, besides the data scaling method, we foucs on dimensionality reduction, which reduce the number of random variables (features) to consider. Useful link: <a href=\"https://scikit-learn.org/stable/\">official documentation</a>, <a href=\"https://scikit-learn.org/stable/modules/decomposition.html#decompositions\">dimensionality reduction in sklearn</a>.\n",
    "2. **Correlation Analysis** aims to investigate the correlations among different numerical properties. Machine Learning algorithms are sensitive to data, and if two features are highly correlated to each other, either of them can be considered redundant (repeated information), e.g., age and date of birth, if a ML model is developed based on both features, it will learn the same thing out of them, which is not only useless but also complicates the model, potentially increasing the time consumption.\n",
    "3. **Principal Component Analysis (PCA)** is used to project the original dataset into a new feature space (the compoments). Simply put, PCA tries to reduce the number of variables, while preserving the characteristics and differences as much as possible. Useful link: <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Wiki</a>, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\">PCA in sklearn</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ab0cb-ebca-49be-9e34-e52dcae1dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed python libraries\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a8596-526f-41d2-b5c7-dea089c762ac",
   "metadata": {},
   "source": [
    "### 1. Tutorial\n",
    "Here we use IRIS dataset as an example to show how you perform data scaling and PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985343b8-5449-440b-bda4-e19deae2ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "iris_data = datasets.load_iris()\n",
    "columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "df_iris = pd.DataFrame(iris_data.data, columns = columns)\n",
    "df_iris['type'] = 'setosa'\n",
    "df_iris.loc[50:99, 'type'] = 'versicolor'\n",
    "df_iris.loc[100:149, 'type'] = 'virginica'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d5bff-83d8-43d7-b79c-7219fa4f18bc",
   "metadata": {},
   "source": [
    "### 1.1 Data scaling\n",
    "Before performing Machine Learning tasks, data must be scaled to belong to similar numerical space (range). Sveral scalers are available, e.g., StandardScaler standardizes features by removing the mean, scaling the data to unit variance (`StandardScaler` from sklean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113e22b-e3e9-42d2-acdf-95d934aedc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original dataset\n",
    "df_iris_copy = df_iris.copy()\n",
    "\n",
    "# define the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# for each column in the dataset, fit and transform the data\n",
    "for col in columns:\n",
    "    \n",
    "    # fit the scaler on the data \n",
    "    scaler.fit(df_iris_copy[col].values.reshape(-1, 1))\n",
    "\n",
    "    # transform the data\n",
    "    df_iris_copy[col] = scaler.transform(df_iris_copy[col].values.reshape(-1, 1))\n",
    "\n",
    "# OR you can do it in an easy way\n",
    "# df_iris_copy[columns] = scaler.fit_transform(df_iris_copy[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d47de-fc44-4909-ba37-65ad10b02b0b",
   "metadata": {},
   "source": [
    "### 1.2 Correlation analysis\n",
    "In order to analyze the correlation, we need to compute the correlation between each pair of features, which can be done using the pandas function ``.corr()`` (<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html?highlight=corr#pandas.DataFrame.corr\">documentation</a>). It calculate the Pearson correlation coefficient (<a href=\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\">Wiki</a>) between two numerical features, which is a value between -1 and 1. Normally, we take the absolute value, and the closer to 1 the higher the correlation.\n",
    "$$ r = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{\\sqrt{[n\\sum x^2 - (\\sum x)^2][n\\sum y^2 - (\\sum y)^2]}} $$\n",
    "<!-- $$ r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2} \\sqrt{\\sum (y_i - \\bar{y})^2}} $$ -->\n",
    "The result of correlation analysis can be displayed in a heatmap (<a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\">documentation</a>), which is a symmetric matrix, indicating the correlation cocoefficient between each pair of features (even a feature with itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8ed7d1-4e0e-46b5-a247-f2c17248b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_iris_copy.drop(columns=['type']).corr().abs()\n",
    "\n",
    "# Compute the heatmap\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(correlation_matrix, cmap='Blues', annot=True, vmin=.0, vmax=1, cbar_kws={'label':'Correlation'})\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d4853-0257-41b6-92b7-d515fa9288a8",
   "metadata": {},
   "source": [
    "### 1.3 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9274b0b-b682-49c9-a6f5-8a6cf0095c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA must be initialized with a random state to initialize the space\n",
    "pca = PCA(random_state=15)\n",
    "\n",
    "# .fit() is used to compute the new dimensions with number of features from 1 to the number of original features\n",
    "pca.fit(df_iris_copy[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ea03b-4906-40e1-87ac-6edd4aeb36e7",
   "metadata": {},
   "source": [
    "### 1.4 PCA visualization\n",
    "HINT: Use a number components where you meet the elbow point. i.e., if increasing the number of components does not increase much the cumulative explained variance. In the title, you can specify the percentage at the number of component you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a818c-b977-4420-9e7d-258bd68f41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe how much of the dataset variability is indicated by a given amount of features\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# evaluate the total dataset variability while increasing the variables\n",
    "cumul_exp_var = np.cumsum(explained_variance)\n",
    "\n",
    "# percentage value to better understand the best number of components\n",
    "perc_cumul_exp_var = cumul_exp_var * 100\n",
    "\n",
    "# make the plot of cumulative explained variance wrt number of components\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(perc_cumul_exp_var, marker='o')\n",
    "plt.xlabel('# Principal Components (PCs)')\n",
    "plt.ylabel('Cumulative explained variance [%]')\n",
    "plt.xticks([i for i in range(4)], [i for i in range(1,5)])\n",
    "plt.grid()\n",
    "plt.title(f'3 PCs explain {round(perc_cumul_exp_var[2], 2)}% of $\\sigma^2$')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ae514-c1f6-4f97-89a2-b37dd25b5983",
   "metadata": {},
   "source": [
    "### 1.5 PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba3929-62dc-4875-a8bc-2177a2d5f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the PCA with the best numbre of components, in this case, it's 3\n",
    "pca = PCA(n_components=3, random_state=15)\n",
    "\n",
    "# fit the data to new space\n",
    "pca.fit(df_iris_copy[columns])\n",
    "\n",
    "# transform the original data into PCA components\n",
    "pca_result = pca.transform(df_iris_copy[columns])\n",
    "\n",
    "# create the new dataset\n",
    "df_iris_pca = pd.DataFrame(pca_result, columns=['component_1', 'component_2', 'component_3'])\n",
    "df_iris_pca['type'] = df_iris['type']\n",
    "df_iris_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f5077-402c-43d6-855c-72aedb7ba748",
   "metadata": {},
   "source": [
    "### 1.6 Loading score\n",
    "The loading score can be used to extract the coefficients of the linear combination of the original variables from which the principal components (PCs) are constructed. It can describe how strongly a component describes the original features and identify potentially redundant features in for a given component. Note that the score is computed as ``pca.components_.T * np.sqrt(pca.explained_variance_)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23224179-8cb9-4109-8130-a2b43be97531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loading scores and create the dataframe\n",
    "loadings = pd.DataFrame(\n",
    "    data = pca.components_.T * np.sqrt(pca.explained_variance_), \n",
    "    columns = [f'PC{i}' for i in range(1, 4)],\n",
    "    index = columns\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "loadings = loadings[['PC1', 'PC2']]\n",
    "loadings.sort_values(['PC1', 'PC2']).rename(columns={'PC1':'PC$_{1}$', 'PC2':'PC$_{2}$'}).plot.barh(ax=plt.gca())\n",
    "plt.grid()\n",
    "plt.xlabel('Loadings')\n",
    "plt.ylabel('Feature')\n",
    "plt.title(f'3 PCs explain {round(perc_cumul_exp_var[2], 2)}% of $\\sigma^2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d22fd0-ea7a-4935-8158-c6c75a4feb00",
   "metadata": {},
   "source": [
    "### 2. Exercise - Darknet Dataset\n",
    "In this lab, we still focus on Darknet dataset (refer to the introduction of previous lab). Specifically, you will develop the ML pipeline to extract features from the dataset, understand the features, and perform a Principal Component Analysis (PCA).\n",
    "\n",
    "In the ``darknet_traces.csv`` file, you will find logs of darknet traffic. Each record contains the following features:\n",
    "- ts: timestamp of the received packet\n",
    "- src_ip: IP address from which the packet was sent\n",
    "- src_port: source port from which the packet was sent\n",
    "- dst_ip: Darknet IP address that the packet reached\n",
    "- dst_port: darknet port the packet reached\n",
    "- proto: protocol used\n",
    "- pck_len: Length of the packet in bytes\n",
    "- ttl: packet time to live"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbedc32f-d3b4-4321-ac52-27e21661a4d2",
   "metadata": {},
   "source": [
    "### 2.1 Dataset management\n",
    "1. The provided features are not suitable for our task. We need to treat it as the source data to generate a new dataset with new features calculated from available features. Specifically, you need to generate a dataset reporting the information of source IP address, and for each individual source IP address (a row), you need to get the following 16 features (Fill NaN values with 0s):\n",
    "    - Number of packets\n",
    "    - Number of different ports contacted\n",
    "    - Number of distinct used protocols\n",
    "    - Number of distinct contacted `dst_ip`\n",
    "    - Min, Max, Avg, Std of the `dst_port`, treated as decimal number\n",
    "    - Min, Max, Avg, Std of the `pck_len`\n",
    "    - Min, Max, Avg, Std of the `ttl`\n",
    "2. Use a standard scalar to scale the entire dataset.\n",
    "3. Create two plots, randomly selecting a subset of the dataset (you can randomly select 1000 indices):\n",
    "    - The first shows the packets sent (x-axis) and the ports (y-axis) contacted.\n",
    "    - The second shows the scaled version of the packets sent and contacted ports contacted.\n",
    "    - Answering the following question:\n",
    "        - Are these graphs similar, why?\n",
    "        - If you re-run the script (different random selection), are they still similar?\n",
    "        - What is the impact of standardization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a7409-59e8-46ab-a90c-9065e0726119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('darknet_traces.csv')\n",
    "\n",
    "# Add one column named 'packets', containing only 1 packet for each row\n",
    "df['packets'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c7018-3e7a-4e7d-b4bf-b5ab3ce24f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57c32a-741e-4a66-8c16-242cdc9b03dd",
   "metadata": {},
   "source": [
    "### 2.2 Correlation analysis\n",
    "- For the derived dataset with new features, analyze the correlation and output the heatmap with correlation coefficient, keeping only two digits. Answering the following questions:\n",
    "    - How many features have highly correlated other features?\n",
    "    - How many features can you remove?\n",
    "    - When one feature is correlated to more than 1 other feature, how do you decide which to remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc68f07-ee69-474c-9e9b-64309d5a46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddda44-5386-4bc1-af90-2e8ffca8658a",
   "metadata": {},
   "source": [
    "### 2.3 PCA\n",
    "Let us begin by examining dimensionality reduction and how the number of components affects PCA’s ability to describe the dataset. Note that we do not eleminate correlated features for now, but focus on the original standardized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8eba1-3ab1-4c48-9ada-f0ca497514d4",
   "metadata": {},
   "source": [
    "- Fit the PCA with the scaled dataset by setting ``random_state=15``. Examine the explained variance and plot the trend of the cumulative explained variance as you increase the number of components created. How many components would you choose? And why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6373fb-15bb-4cfe-a022-9e635fc675ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19463b0c-4d24-4baa-894b-dd13936db16d",
   "metadata": {},
   "source": [
    "- From now on, keep the first 10 components, initializing PCA and fitting as well as transforming the dataset again. What is the cumulative explained variance. Is it identical to the one given in the plot of cumulative explained variance, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13219c-496d-41b7-950a-48f9463c9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ff79d-4745-40c9-817a-673237f33870",
   "metadata": {},
   "source": [
    "- Consider only the first 2 principal components (PC1 and PC2) of the 10 selected. Compute the Loading score for each of the original features with respect to the selected components. Notice that you can calculate the score as follows: <br>``scores = pca.components_.T * np.sqrt(pca.explained_variance_)``<br>\n",
    "Remember that the loading scores can be seen as the weights of the each original feature used to generate the PCs. Answering the following questions:\n",
    "    - Which feature contributes the most to PC1? And for PC2?\n",
    "    - What are the most redundant features accordingly to the selected components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68805b1c-5a50-4144-bfc5-0716dbee5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2694b04-13d9-4c05-b2de-1d5afc29deee",
   "metadata": {},
   "source": [
    "### 2.4 Repeat correlation analysis after performing PCA\n",
    "For the transformed data with the optimal number of components, repeat the process of correlation heatmap. Answering the following questions:\n",
    "- How many features (components) can you remove now, after doing the PCA?\n",
    "- What phenomena can you observe? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cf5ae-7e4c-4af3-841f-78ac4b7971cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139325bf-60a5-49ce-936f-855fa461caa7",
   "metadata": {},
   "source": [
    "### 2.5 Optional - PCA for features without standardization\n",
    "Previously, we have done the whole procedure following the order from standardization to PCA, but what if we perform the PCA without standardizing the dataset? Herein, you repeat what you have done in 2.2, except that you refer to the original dataset with raw features instead of performing standardization.\n",
    "1. Fit the PCA with the **original** features. Examine the explained variance and plot the trend of the cumulative explained variance as you have done before. Answering the following questions:\n",
    "    - How many components would you choose now? Why?\n",
    "    - Is the new result different from the previous result? Why?\n",
    "2. Based on the selected number of components, re-fit the PCA on the original features.\n",
    "3. Repeat the calculation and visualization of Loading score for the first 2 principal components (PC1 and PC2). Answering the following questions:\n",
    "    - Which feature contributes the most to PC1? And for PC2?\n",
    "    - Do you observe something abnormal? Why? How do you visualize them (tip: change the scale of reference frame)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6d6b7-0327-4e6c-ac97-f3dc18e9c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1507c8-6670-448b-888b-c2095b60ddec",
   "metadata": {},
   "source": [
    "### 2.6 Optional - Correlation analysis with Spearman's rank correlation coefficient\n",
    "Previously, we have done the correlation analysis using Pearson correlation coefficient, and now, you can choose to explore the Spearman correlation coefficient, by specifying the ``method`` argument in ``corr()`` function. Note that you need to use the original standardized dataset instead of the one after performing PCA. Answering the following questions:\n",
    "- What differences can you observe with respect to the previous correlation analysis?\n",
    "- What similarities can you observe with respect to the previous correlation analysis?\n",
    "- How many features can you remove now? Is it same as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ca1e9-c0ba-451f-9d5e-4b38f7c240f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
